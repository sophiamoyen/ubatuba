# -*- coding: utf-8 -*-
"""
-----------------------------------------------------------------------
Automatic detection of onset of seizures on EEG datasets using Discrete
Wavelet Transform(DWT), statistical features, Mutual Information Gain 
and Random Forest

Authors:
Emanuel Iwanow de Araujo
Sophia Bianchi Moyen
Michael
-----------------------------------------------------------------------
"""

"""
--------------------------------------------------------
Importing libraries and functions
--------------------------------------------------------
"""
# Basic libraries
import numpy as np
import pandas as pd

# From the WKI competition
from wettbewerb import load_references, get_3montages, get_6montages

# Signal/Dataset processing
import mne
from scipy import signal as sig
from scipy.stats import kurtosis, skew
import antropy as an
import pywt
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

# Feature Analysis
from sklearn.feature_selection import SelectKBest, mutual_info_classif

# Classification Model
from sklearn.ensemble import RandomForestClassifier

# Libraries to save/open model
import os
import joblib
import json
import csv


### if __name__ == '__main__':  # bei multiprocessing auf Windows notwendig
"""
---------------------------------------------------------------------------
Loading Dataset
---------------------------------------------------------------------------
"""
training_folder  = "../../training"

print('Loading Dataset')
ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(training_folder) # Importiere EEG-Dateien, zugehÃ¶rige Kanalbenennung, Sampling-Frequenz (Hz) und Name (meist fs=256 Hz), sowie Referenzsystem
print('Dataset loaded')

"""
---------------------------------------------------------------------
Parameters 
---------------------------------------------------------------------
"""
number_montages = 6        # Choose between using 6 or 3 montages
N_samples = 2000           # Number of samples per division
wavelet = 'db4'            # Wavelet tyoe used for DWT
num_coef_dwt = 5           # Number of output coefficients for DWT
num_features = 1           # Number of statistical features extracted
scaler = StandardScaler()  # Scaler chosen for normalization of signal

# Empty list to store features overall
features = []      

for i,_id in enumerate(ids):
    """
    ------------- Generating Statistical Features from Epochs ---------------

    """
    
    if number_montages == 6:
        _montage, _montage_data, _is_missing = get_6montages(channels[i], data[i])
    else:
        _montage, _montage_data, _is_missing = get_3montages(channels[i], data[i])
        
    # Getting frequency used for sampling in signal in dataset
    _fs = sampling_frequencies[i]
    
    # Empty list to store features per signal in dataset
    features_per_id = []

    for j, signal_name in enumerate(_montage):
        features_per_mont = []
        signal = _montage_data[j]
        
        """
        -------------------- Pre-Processing ---------------------------
        """
        # Notch-Filter to compensate net frequency of 50 Hz and its harmonic 100 Hz
        signal_notch = mne.filter.notch_filter(x=signal, Fs=_fs, freqs=np.array(50), n_jobs=2, verbose=False)
        # Bandpassfilter between 0.5Hz and 70Hz to filter out noise
        signal_filter = mne.filter.filter_data(data=signal_notch, sfreq=_fs, l_freq=3, h_freq=40.0, n_jobs=2, verbose=False)
        # Defining number of divisions for signal according to user-defined N_samples per Epoch
        N_div = len(signal_filter)//N_samples
        # Normalizing data
        norm_montage_data = scaler.fit_transform(signal_filter.reshape(-1,1)).reshape(1,-1)[0]
    
        for i in range(N_div):
            """
            ----------------- Discrete Wavelet Decomposition ----------
            """
            # Creates empty array to store features for this Epoch
            features_per_div = np.zeros(num_features*num_coef_dwt)
            
            # Create Epoch for pre-processing
            montage_array = norm_montage_data[i*N_samples:(i+1)*N_samples]
            
            # Extracting coefficients from Discrete Wavelet Decomposition (DWT)
            ca4, cd4, cd3, cd2, cd1 = pywt.wavedec(montage_array, wavelet, level=4)
            montage_dwt = [ca4, cd4, cd3, cd2, cd1]
            
            m = 0 # Support variable for inserting features in designated location in array
            for signal_coefficient in montage_dwt:
                """
                --------------------- Feature calculation -------------------------
                For each coefficient signal (CA4, CD4, CD3, CD2, CD1) generated by the DWT,
                calculates the feature Line Length
                --------------------------------------------------------------------
                """
                # Calculates the LINE LENGTH feature of the decomposed signal
                line_length = np.sum(np.abs(np.diff(signal_coefficient)))/len(signal_coefficient) 
                features_per_div[m] = line_length 
                
                """
                # Calculates the SKEWNESS of the decomposed signal
                skewness = skew(signal_coefficient.tolist()) 
                # Calculates the KURTOSIS of the decomposed signal
                kurt = kurtosis(signal_coefficient.tolist()) 
                
                
                # Insert features in array of features for this Epoch
                features_per_div[m:(m+3)] = np.array([line_length,
                                                      skewness, 
                                                      kurt])
                """
                m += 1 # Jumps 3 positions in array
                
            # Adds features colected for each Epoch to each Montage array
            features_per_mont.append(features_per_div)
    
        # Adds features from each montage to list of features overall
        features_per_id.append(features_per_mont)
        
    # Formatting lists of features collected into one list
    for n_piece in range(len(features_per_mont)):
        list_montages = []
        for n_montage in range(len(features_per_id)):
            list_montages += features_per_id[n_montage][n_piece].flatten().tolist()
        features.append(np.array(list_montages))

"""
-------------------------------------------------------------------------
Relabling dataset based on Epochs acquired from signal windowing
-------------------------------------------------------------------------
"""
labels = []
for i,_id in enumerate(ids):
    if eeg_labels[i][0]:
    # If signal contains seizure -> relabels each epoch according to where seizure happens
        onset = eeg_labels[i][1]                 # Getting onset
        offset = eeg_labels[i][2]                # Getting offset
        sample_freq = sampling_frequencies[i]    # Getting frequency 
        total_time = len(data[i][1])/sample_freq # Getting length of signal in seconds
        N_div = len(data[i][1])//N_samples       # Getting number of Epochs for that signal
        
        for num in range(N_div):
            """
            If Epoch starts before/at onset and seizure continues until end of Epoch 
            OR
            If Epoch starts after/at onset and ends before offset

            >> THEN >>             
            Label Epoch as having seizure
            """
            start_epoch_timestep = (total_time/N_div)*(num)
            end_epoch_timestep = (total_time/N_div)*(num+1)
            if ((start_epoch_timestep <= onset) and (end_epoch_timestep > onset)) or ((start_epoch_timestep >= onset) and (start_epoch_timestep < offset)):
                # Seizure present in Epoch
                labels.append([1])
            else:
                # No seizure present in Epoch
                labels.append([0])
                
    else:
    # If signal doesn't contain seizure -> label = False all Epochs
        N_div = len(data[i][1])//N_samples
        for num in range(N_div):
            labels.append([0])
            
# Reshape labels array
labels = np.reshape(labels, (1,-1))[0]

"""
------------------------------------------------------------------
Visualizing Mutual Information Gain from Features
------------------------------------------------------------------
"""

if number_montages == 6:
    column_values = ['M1_ca4_ll', 'M1_cd4_ll', 'M1_cd3_ll', 'M1_cd2_ll', 'M1_ca1_ll', 
                     'M2_ca4_ll', 'M2_cd4_ll', 'M2_cd3_ll', 'M2_cd2_ll', 'M2_ca1_ll', 
                     'M3_ca4_ll', 'M3_cd4_ll', 'M3_cd3_ll', 'M3_cd2_ll', 'M3_ca1_ll', 
                     'M4_ca4_ll', 'M4_cd4_ll', 'M4_cd3_ll', 'M4_cd2_ll', 'M4_ca1_ll', 
                     'M5_ca4_ll', 'M5_cd4_ll', 'M5_cd3_ll', 'M5_cd2_ll', 'M5_ca1_ll', 
                     'M6_ca4_ll', 'M6_cd4_ll', 'M6_cd3_ll', 'M6_cd2_ll', 'M6_ca1_ll']

else:
    column_values = ['M1_ca4_ll', 'M1_cd4_ll', 'M1_cd3_ll', 'M1_cd2_ll', 'M1_ca1_ll', 
                     'M2_ca4_ll', 'M2_cd4_ll', 'M2_cd3_ll', 'M2_cd2_ll', 'M2_ca1_ll', 
                     'M3_ca4_ll', 'M3_cd4_ll', 'M3_cd3_ll', 'M3_cd2_ll', 'M3_ca1_ll']
    
# Creating Dataframe to see features
df_features = pd.DataFrame(data = features,
                          columns = column_values)


mutual_info = mutual_info_classif(features, labels)
mutual_info = pd.Series(mutual_info)
mutual_info.index = df_features.columns
mutual_info = mutual_info.sort_values(ascending=False)
print(mutual_info)

from pathlib import Path
filepath = Path('mutual_info.csv')  
filepath.parent.mkdir(parents=True, exist_ok=True)  
mutual_info.to_csv(filepath, header=None)  


"""
-----------------------------------------------------------------
Dealing with Class Imbalance
-----------------------------------------------------------------
"""
oversample = SMOTE()
features_smote, labels_smote = oversample.fit_resample(features,labels)

"""
-----------------------------------------------------------------
Random Forest Classifier 
-----------------------------------------------------------------
"""
rf_classifier = RandomForestClassifier( n_estimators=300,     # Number of trees in the forest
                                        max_features="sqrt",  # Number of features to consider at each split
                                        max_depth=10,         # Maximum depth of each tree
                                        min_samples_leaf=1,   # Minimum number of samples required to be at a leaf node
                                        )

rf_classifier.fit(features_smote, labels_smote)


"""
-----------------------------------------------------------------
Save trained Model
-----------------------------------------------------------------
"""
joblib.dump(rf_classifier, 'model.joblib') 

