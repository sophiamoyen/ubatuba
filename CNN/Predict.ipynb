{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06f2662-6df6-4aed-9ffe-10d91f8f2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Diese Datei sollte nicht verändert werden und wird von uns gestellt und zurückgesetzt.\n",
    "\n",
    "Funktionen zum Laden und Speichern der Dateien\n",
    "\"\"\"\n",
    "__author__ = \"Maurice Rohr und Dirk Schweickard\"\n",
    "\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import csv\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "### Achtung! Diese Funktion nicht veraendern.\n",
    "def load_references(folder: str = '../../training') -> Tuple[List[str], List[List[str]],\n",
    "                                                          List[np.ndarray],  List[float],\n",
    "                                                          List[str], List[Tuple[bool,float,float]]]:\n",
    "    \"\"\"\n",
    "    Liest Referenzdaten aus .mat (Messdaten) und .csv (Label) Dateien ein.\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : str, optional\n",
    "        Ort der Trainingsdaten. Default Wert '../training'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ids : List[str]\n",
    "        Liste von ID der Aufnahmen\n",
    "    channels : List[List[str]]\n",
    "        Liste der vorhandenen Kanäle per Aufnahme\n",
    "    data :  List[ndarray]\n",
    "        Liste der Daten pro Aufnahme\n",
    "    sampling_frequencies : List[float]\n",
    "        Liste der Sampling-Frequenzen.\n",
    "    reference_systems : List[str]\n",
    "        Liste der Referenzsysteme. \"LE\", \"AR\", \"Sz\" (Zusatz-Information)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialisiere Listen ids, channels, data, sampling_frequencies, refernece_systems und eeg_labels\n",
    "    ids: List[str] = []\n",
    "    channels: List[List[str]] = []\n",
    "    data: List[np.ndarray] = []\n",
    "    sampling_frequencies: List[float] = []\n",
    "    reference_systems: List[str] = []\n",
    "    eeg_labels: List[Tuple[bool,float,float]] = []\n",
    "    \n",
    "    # Erzeuge Datensatz aus Ordner und fülle Listen mit Daten\n",
    "    dataset = EEGDataset(folder)\n",
    "    for item in dataset:\n",
    "        ids.append(item[0])\n",
    "        channels.append(item[1])\n",
    "        data.append(item[2])\n",
    "        sampling_frequencies.append(item[3])\n",
    "        reference_systems.append(item[4])\n",
    "        eeg_labels.append(item[5])\n",
    "        \n",
    "    # Zeige an wie viele Daten geladen wurden\n",
    "    print(\"{}\\t Dateien wurden geladen.\".format(len(ids)))\n",
    "    return ids, channels, data, sampling_frequencies, reference_systems, eeg_labels\n",
    "\n",
    "### Achtung! Diese Klasse nicht veraendern.\n",
    "class EEGDataset:\n",
    "    def __init__(self,folder:str) -> None:\n",
    "        \"\"\"Diese Klasse stellt einen EEG Datensatz dar.\n",
    "        \n",
    "        Verwendung:\n",
    "            Erzeuge einen neuen Datensatz (ohne alle Daten zu laden) mit\n",
    "            dataset = EEGDataset(\"../training/\")\n",
    "            len(dataset) # gibt Größe des Datensatzes zurück\n",
    "            dataset[0] # gibt erstes Element aus Datensatz zurück bestehend aus (id, channels, data, sampling_frequency, reference_system, eeg_label)\n",
    "            it = iter(dataset) # gibt einen iterator zurück auf den Datensatz,\n",
    "            next(it) # gibt nächstes Element zurück bis alle Daten einmal geholt wurden\n",
    "            for item in dataset: # iteriert einmal über den gesamten Datensatz\n",
    "                (id, channels, data, sampling_frequency, reference_system, eeg_label) = item\n",
    "                # Berechnung\n",
    "\n",
    "        Args:\n",
    "            folder (str): Ordner in dem der Datensatz bestehend aus .mat-Dateien und einer REFERENCE.csv Datei liegt\n",
    "        \"\"\"\n",
    "        assert isinstance(folder, str), \"Parameter folder muss ein string sein aber {} gegeben\".format(type(folder))\n",
    "        assert os.path.exists(folder), 'Parameter folder existiert nicht!'\n",
    "        # Initialisiere Listen für ids und labels\n",
    "        self._folder = folder\n",
    "        self._ids: List[str] = []\n",
    "        self._eeg_labels: List[Tuple[bool,float,float]] = []\n",
    "        # Lade references Datei\n",
    "        with open(os.path.join(folder, 'REFERENCE.csv')) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            # Iteriere über jede Zeile\n",
    "            for row in csv_reader:\n",
    "                self._ids.append(row[0])\n",
    "                self._eeg_labels.append((int(row[1]),float(row[2]),float(row[3])))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._ids)\n",
    "    \n",
    "    def __getitem__(self,idx) -> Tuple[str, List[str],\n",
    "                                    np.ndarray,  float,\n",
    "                                    str, Tuple[bool,float,float]]:\n",
    "        #Lade Matlab-Datei\n",
    "        eeg_data = sio.loadmat(os.path.join(self._folder, self._ids[idx] + '.mat'),simplify_cells=True)\n",
    "        ch_names = eeg_data.get('channels')\n",
    "        channels = [x.strip(' ') for x in ch_names] \n",
    "        data = eeg_data.get('data')\n",
    "        sampling_frequency = eeg_data.get('fs')\n",
    "        reference_system = eeg_data.get('reference_system')\n",
    "        return (self._ids[idx],channels,data,sampling_frequency,reference_system,self._eeg_labels[idx])\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self._eeg_labels\n",
    "    \n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Achtung! Diese Funktion nicht veraendern.\n",
    "#predictions = {\"id\":id,\"seizure_present\":seizure_present,\"seizure_confidence\":seizure_confidence,\n",
    "#                   \"onset\":onset,\"onset_confidence\":onset_confidence,\"offset\":offset,\n",
    "#                   \"offset_confidence\":offset_confidence}\n",
    "def save_predictions(predictions: List[Dict[str,Any]], folder: str=None) -> None:\n",
    "    \"\"\"\n",
    "    Funktion speichert the gegebenen predictions in eine CSV-Datei mit dem name PREDICTIONS.csv. \n",
    "    Alle Optionalen Vorherhsagen werden mit Standardwerten ersetzt.\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : List[Dict[str,Any]]\n",
    "        Liste aus dictionaries. Jedes Dictionary enthält die Felder \"id\",\"seizure_present\",\n",
    "                \"seizure_confidence\" (optional),\"onset\",\"onset_confidence\" (optional),\n",
    "                \"offset\" (optional),\"offset_confidence\" (optional)\n",
    "\tfolder : str\n",
    "\t\tSpeicherort der predictions\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"    \n",
    "\t# Check Parameter\n",
    "    assert isinstance(predictions, list), \\\n",
    "        \"Parameter predictions muss eine Liste sein aber {} gegeben.\".format(type(predictions))\n",
    "    assert len(predictions) > 0, 'Parameter predictions muss eine nicht leere Liste sein.'\n",
    "    assert isinstance(predictions[0], dict), \\\n",
    "        \"Elemente der Liste predictions muss ein Dictionary sein aber {} gegeben.\".format(type(predictions[0]))\n",
    "    assert \"id\" in predictions[0], \\\n",
    "        \"Prädiktionen müssen eine ID besitzen, aber Key in Dictionary nicht vorhanden\"\n",
    "\t\n",
    "    if folder==None:\n",
    "        file = \"PREDICTIONS.csv\"\n",
    "    else:\n",
    "        file = os.path.join(folder, \"PREDICTIONS.csv\")\n",
    "    # Check ob Datei schon existiert wenn ja loesche Datei\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "    with open(file, mode='w', newline='') as predictions_file:\n",
    "\n",
    "        # Init CSV writer um Datei zu beschreiben\n",
    "        predictions_writer = csv.writer(predictions_file, delimiter=',')\n",
    "        # Iteriere über jede prediction\n",
    "        header=[\"id\",\"seizure_present\",\"seizure_confidence\",\"onset\",\"onset_confidence\",\"offset\",\"offset_confidence\"]\n",
    "        predictions_writer.writerow(header)\n",
    "        for prediction in predictions:\n",
    "            _id = prediction[\"id\"]\n",
    "            _seizure_present = prediction[\"seizure_present\"]\n",
    "            _seizure_confidence = prediction.get(\"seizure_confidence\",1.0) \n",
    "            _onset = prediction[\"onset\"]\n",
    "            _onset_confidence = prediction.get(\"onset_confidence\",1.0) \n",
    "            _offset = prediction.get(\"offset\",999999.0)\n",
    "            _offset_confidence = prediction.get(\"offset_confidence\",0.0)\n",
    "            predictions_writer.writerow([_id,_seizure_present,_seizure_confidence,_onset,_onset_confidence,_offset,_offset_confidence])\n",
    "        # Gebe Info aus wie viele labels (predictions) gespeichert werden\n",
    "        print(\"{}\\t Labels wurden geschrieben.\".format(len(predictions)))\n",
    "        \n",
    "\n",
    "def get_3montages(channels: List[str], data: np.ndarray) -> Tuple[List[str],np.ndarray,bool]:\n",
    "    \"\"\"\n",
    "    Funktion berechnet die 3 Montagen Fp1-F3, Fp2-F4, C3-P3 aus den gegebenen Ableitungen (Montagen)\n",
    "    zur selben Referenzelektrode. Falls nicht alle nötigen Elektroden vorhanden sind, wird das entsprechende Signal durch 0 ersetzt. \n",
    "    ----------\n",
    "    channels : List[str]\n",
    "        Namen der Kanäle z.B. Fp1, Cz, C3\n",
    "\tdata : ndarray\n",
    "\t\tDaten der Kanäle\n",
    "    Returns\n",
    "    -------\n",
    "    montages : List[str]\n",
    "        Namen der Montagen [\"Fp1-F3\", \"Fp2-F4\", \"C3-P3\"]\n",
    "    montage_data : ndarray\n",
    "        Daten der Montagen\n",
    "    montage_missing : bool\n",
    "        1 , falls eine oder mehr Montagen fehlt, sonst 0\n",
    "\n",
    "    \"\"\"   \n",
    "    montages = []\n",
    "    _,m = np.shape(data)\n",
    "    montage_data = np.zeros([3,m])\n",
    "    montage_missing = 0\n",
    "    if '-' in channels:\n",
    "        try:\n",
    "            montage_data[0,:] = data[channels.index('Fp1-F3')]\n",
    "            montages.append('Fp1-F3')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "        try:\n",
    "            montage_data[1,:] = data[channels.index('Fp2-F4')]\n",
    "            montages.append('Fp2-F4')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')        \n",
    "        try:\n",
    "            montage_data[2,:] = data[channels.index('C3-P3')]\n",
    "            montages.append('C3-P3')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "\n",
    "        return (montages,montage_data,montage_missing)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            montage_data[0,:] = data[channels.index('Fp1')] - data[channels.index('F3')]\n",
    "            montages.append('Fp1-F3')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "        try:\n",
    "            montage_data[1,:] = data[channels.index('Fp2')] - data[channels.index('F4')]\n",
    "            montages.append('Fp2-F4')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "        try:\n",
    "            montage_data[2,:] = data[channels.index('C3')] - data[channels.index('P3')]\n",
    "            montages.append('C3-P3')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "\n",
    "        return (montages,montage_data,montage_missing)\n",
    "\n",
    "\n",
    "def get_6montages(channels: List[str], data: np.ndarray) -> Tuple[List[str],np.ndarray,bool]:\n",
    "    \"\"\"\n",
    "    Funktion berechnet die 6 Montagen Fp1-F3, Fp2-F4, C3-P3, F3-C3, F4-C4, C4-P4 aus den gegebenen Ableitungen (Montagen)\n",
    "    zur selben Referenzelektrode. Falls nicht alle nötigen Elektroden vorhanden sind, wird das entsprechende Signal durch 0 ersetzt. \n",
    "    ----------\n",
    "    channels : List[str]\n",
    "        Namen der Kanäle z.B. Fp1, Cz, C3\n",
    "\tdata : ndarray\n",
    "\t\tDaten der Kanäle\n",
    "    Returns\n",
    "    -------\n",
    "    montages : List[str]\n",
    "        Namen der Montagen [\"Fp1-F3\", \"Fp2-F4\", \"C3-P3\", \"F3-C3\", \"F4-C4\", \"C4-P4\"]\n",
    "    montage_data : ndarray\n",
    "        Daten der Montagen\n",
    "    montage_missing : bool\n",
    "        1 , falls eine oder mehr Montagen fehlt, sonst 0\n",
    "\n",
    "    \"\"\"  \n",
    "    montages = []\n",
    "    _,m = np.shape(data)\n",
    "    montage_data = np.zeros([6,m])\n",
    "    montage_missing = 0\n",
    "    if '-' in channels:\n",
    "        try:\n",
    "            montage_data[0,:] = data[channels.index('Fp1-F3')]\n",
    "            montages.append('Fp1-F3')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "        try:\n",
    "            montage_data[1,:] = data[channels.index('Fp2-F4')]\n",
    "            montages.append('Fp2-F4')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')        \n",
    "        try:\n",
    "            montage_data[2,:] = data[channels.index('C3-P3')]\n",
    "            montages.append('C3-P3')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "        try:\n",
    "            montage_data[3,:] = data[channels.index('F3-C3')]\n",
    "            montages.append('F3-C3')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "        try:\n",
    "            montage_data[4,:] = data[channels.index('F4-C4')]\n",
    "            montages.append('F4-C4')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')        \n",
    "        try:\n",
    "            montage_data[5,:] = data[channels.index('C4-P4')]\n",
    "            montages.append('C4-P4')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "\n",
    "        return (montages,montage_data,montage_missing)\n",
    "\n",
    "    else:         \n",
    "        try:\n",
    "            montage_data[0,:] = data[channels.index('Fp1')] - data[channels.index('F3')]\n",
    "            montages.append('Fp1-F3')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "        try:\n",
    "            montage_data[1,:] = data[channels.index('Fp2')] - data[channels.index('F4')]\n",
    "            montages.append('Fp2-F4')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "        try:\n",
    "            montage_data[2,:] = data[channels.index('C3')] - data[channels.index('P3')]\n",
    "            montages.append('C3-P3')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "        try:\n",
    "            montage_data[3,:] = data[channels.index('F3')] - data[channels.index('C3')]\n",
    "            montages.append('F3-C3')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "        try:\n",
    "            montage_data[4,:] = data[channels.index('F4')] - data[channels.index('C4')]\n",
    "            montages.append('F4-C4')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "        try:\n",
    "            montage_data[5,:] = data[channels.index('C4')] - data[channels.index('P4')]\n",
    "            montages.append('C4-P4')\n",
    "        except:\n",
    "            montage_missing = 1\n",
    "            montages.append('error')\n",
    "\n",
    "        return (montages,montage_data,montage_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2a977d-6ec9-4d9b-a7f7-dc58e2325366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "Skript testet das vortrainierte Modell\n",
    "\n",
    "\n",
    "@author:  Maurice Rohr, Dirk Schweickard\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from wettbewerb import get_3montages\n",
    "\n",
    "# Pakete aus dem Vorlesungsbeispiel\n",
    "import mne\n",
    "from scipy import signal as sig\n",
    "import ruptures as rpt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from wettbewerb import load_references, get_3montages, get_6montages\n",
    "import mne\n",
    "from scipy import signal as sig\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "###Signatur der Methode (Parameter und Anzahl return-Werte) darf nicht verändert werden\n",
    "def predict_labels(channels : List[str], data : np.ndarray, fs : float, reference_system: str, model_name : str='checkpoint.pt') -> Dict[str,Any]:\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    channels : List[str]\n",
    "        Namen der übergebenen Kanäle\n",
    "    data : ndarray\n",
    "        EEG-Signale der angegebenen Kanäle\n",
    "    fs : float\n",
    "        Sampling-Frequenz der Signale.\n",
    "    reference_system :  str\n",
    "        Welches Referenzsystem wurde benutzt, \"Bezugselektrode\", nicht garantiert korrekt!\n",
    "    model_name : str\n",
    "        Name eures Models,das ihr beispielsweise bei Abgabe genannt habt. \n",
    "        Kann verwendet werden um korrektes Model aus Ordner zu laden\n",
    "    Returns\n",
    "    -------\n",
    "    prediction : Dict[str,Any]\n",
    "        enthält Vorhersage, ob Anfall vorhanden und wenn ja wo (Onset+Offset)\n",
    "    '''\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Euer Code ab hier  \n",
    "\n",
    "    # Initialisiere Return (Ergebnisse)\n",
    "    seizure_present = True # gibt an ob ein Anfall vorliegt\n",
    "    seizure_confidence = 0.5 # gibt die Unsicherheit des Modells an (optional)\n",
    "    onset = 4.2   # gibt den Beginn des Anfalls an (in Sekunden)\n",
    "    onset_confidence = 0.99 # gibt die Unsicherheit bezüglich des Beginns an (optional)\n",
    "    offset = 999999  # gibt das Ende des Anfalls an (optional)\n",
    "    offset_confidence = 0   # gibt die Unsicherheit bezüglich des Endes an (optional)\n",
    "\n",
    "\n",
    "# CNN-Modell\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, num_classes, seq_length):\n",
    "            super().__init__()\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=3, out_channels=6, kernel_size=5),\n",
    "                nn.BatchNorm1d(num_features=6),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "                nn.Conv1d(6, 16, 5),\n",
    "                nn.BatchNorm1d(16),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2, 2),\n",
    "            )\n",
    "\n",
    "            # Anpassung für die Berechnung der Größe des linearen Layers\n",
    "            linear_input_size = self._get_conv_output(seq_length)\n",
    "\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(linear_input_size, 120),\n",
    "                nn.BatchNorm1d(120),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(120, 84),\n",
    "                nn.BatchNorm1d(84),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(84, num_classes),\n",
    "            )\n",
    "\n",
    "        def _get_conv_output(self, shape):\n",
    "            with torch.no_grad():\n",
    "                input = torch.zeros(1, 3, shape)\n",
    "                output = self.classifier(input)\n",
    "                return output.numel()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.classifier(x)\n",
    "            x = torch.flatten(x, start_dim=1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "    \n",
    "    cnn_classifier = CNN(num_classes=2, seq_length=2000)\n",
    "    checkpoint = torch.load(model_name)\n",
    "    cnn_classifier.load_state_dict(checkpoint)\n",
    "    cnn_classifier.eval()\n",
    "\n",
    "    \n",
    "    number_montages = 3\n",
    "    N_samples = 2000 # Number of samples per division\n",
    "    # Decompose the wave\n",
    "    wavelet = 'db4'\n",
    "    scaler = StandardScaler()\n",
    "    new_signal = []\n",
    "\n",
    "    mont1_signal = []\n",
    "    mont2_signal = []\n",
    "    mont3_signal = []\n",
    "    whole_mont = [mont1_signal,mont2_signal,mont3_signal]\n",
    "    for i,_id in enumerate(ids):\n",
    "    \n",
    "        if number_montages == 6:\n",
    "            _montage, _montage_data, _is_missing = get_6montages(channels[i], data[i])\n",
    "        else:\n",
    "            _montage, _montage_data, _is_missing = get_3montages(channels[i], data[i])\n",
    "        \n",
    "        _fs = sampling_frequencies[i]\n",
    "        features_per_id = []\n",
    "\n",
    "        for j, signal_name in enumerate(_montage):\n",
    "            signal = _montage_data[j]\n",
    "            # Notch-Filter to compensate net frequency of 50 Hz\n",
    "            signal_notch = mne.filter.notch_filter(x=signal, Fs=_fs, freqs=np.array([50.,100.]), n_jobs=2, verbose=False)\n",
    "            # Bandpassfilter between 0.5Hz and 70Hz to filter out noise\n",
    "            signal_filter = mne.filter.filter_data(data=signal_notch, sfreq=_fs, l_freq=0.5, h_freq=70.0, n_jobs=2, verbose=False)\n",
    "            # Defining number of divisions for signal\n",
    "            N_div = len(signal_filter)//N_samples\n",
    "            # Normalizing data\n",
    "            norm_montage_data = scaler.fit_transform(signal_filter.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    \n",
    "            for i in range(N_div):\n",
    "                montage_array = norm_montage_data[i*N_samples:(i+1)*N_samples]\n",
    "                whole_mont[j].append(montage_array)\n",
    "\n",
    "\n",
    "    labels = []\n",
    "    for i,_id in enumerate(ids):\n",
    "        if eeg_labels[i][0]:\n",
    "            onset = eeg_labels[i][1]\n",
    "            offset = eeg_labels[i][2]\n",
    "            sample_freq = sampling_frequencies[i]\n",
    "            total_time = len(data[i][1])/sample_freq\n",
    "            N_div = len(data[i][1])//N_samples\n",
    "            for num in range(N_div):\n",
    "                if (((total_time/N_div)*(num) <= onset) and ((total_time/N_div)*(num+1) > onset)) or (((total_time/N_div)*(num) >= onset) and ((total_time/N_div)*(num) < offset)):\n",
    "                    labels.append([1])\n",
    "                else:\n",
    "                    labels.append([0])\n",
    "        else:\n",
    "            N_div = len(data[i][1])//N_samples\n",
    "            for num in range(N_div):\n",
    "                labels.append([0])\n",
    "    labels = np.reshape(labels, (1,-1))[0]\n",
    "\n",
    "\n",
    "    # Instanziierung von RandomUnderSampler\n",
    "    undersample = RandomUnderSampler()\n",
    "\n",
    "    # Erstellen Sie eine Funktion, die das Resampling durchführt\n",
    "    def resample_signal(signal, labels):\n",
    "        # Anwenden von RandomUnderSampler\n",
    "        signal_resampled, labels_resampled = undersample.fit_resample(signal, labels)\n",
    "        return signal_resampled, labels_resampled\n",
    "\n",
    "    # Anwenden der Funktion auf jedes Signal\n",
    "    mont1_signal_resampled, labels_resampled_1 = resample_signal(np.array(mont1_signal), labels)\n",
    "    mont2_signal_resampled, labels_resampled_2 = resample_signal(np.array(mont2_signal), labels)\n",
    "    mont3_signal_resampled, labels_resampled_3 = resample_signal(np.array(mont3_signal), labels)\n",
    "\n",
    "    # Stellen Sie sicher, dass die Labels für alle Signale gleich sind, da sie das gleiche Set von Beispielen repräsentieren sollten\n",
    "    assert np.array_equal(labels_resampled_1, labels_resampled_2)\n",
    "    assert np.array_equal(labels_resampled_1, labels_resampled_3)\n",
    "\n",
    "    labels_resampled = labels_resampled_1\n",
    "\n",
    "    whole_mont_resampled = [mont1_signal_resampled,mont2_signal_resampled,mont3_signal_resampled]\n",
    "\n",
    "\n",
    "    whole_mont_resampled_np = np.array(whole_mont_resampled)\n",
    "\n",
    "\n",
    "    # Konvertieren der Daten in PyTorch Tensoren und Vorbereiten für das Modell\n",
    "    data_tensor = torch.tensor(whole_mont_resampled_np, dtype=torch.float).permute(1, 0, 2)  # Permutieren zu [Anzahl der Beispiele, Kanäle, Länge]\n",
    "\n",
    "    # Stellen Sie sicher, dass Ihr Modell und Daten auf dem gleichen Gerät sind\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    cnn_classifier = cnn_classifier.to(device)\n",
    "    data_tensor = data_tensor.to(device)\n",
    "\n",
    "    # Bereiten Sie die Struktur für die Erfassung der Vorhersagen vor\n",
    "    seizure_present_array = np.zeros(data_tensor.shape[0], dtype=bool)\n",
    "\n",
    "    # Vorhersagen durchführen und überprüfen, ob ein Anfall vorhanden ist\n",
    "    with torch.no_grad():\n",
    "        predictions = cnn_classifier(data_tensor)  # Vorhersagen für das gesamte Batch\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)\n",
    "        seizure_indices = torch.where(predicted_classes == 1)[0]  # Angenommen, Klasse 1 steht für \"seizure\"\n",
    "\n",
    "        if len(seizure_indices) > 0:\n",
    "            seizure_present = True\n",
    "            # Erster Anfallindex kann als Anfallsbeginn betrachtet werden\n",
    "            first_seizure_index = seizure_indices[0].item()\n",
    "            onset = N_samples * first_seizure_index / fs  # Berechnung des Anfallsbeginns in Sekunden\n",
    "            offset = N_samples * seizure_indices[-1].item() / fs  # Berechnung des Anfallsendes\n",
    "            # Annahme: Confidence-Werte sind statisch, könnten dynamisch angepasst werden\n",
    "            seizure_confidence = 0.8  # Beispielwert, anpassen basierend auf Ihrer Modellausgabe oder Heuristik\n",
    "            offset_confidence = 0.8  # Beispielwert, anpassen basierend auf Analyse\n",
    "\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "     \n",
    "     \n",
    "    \n",
    "#------------------------------------------------------------------------------  \n",
    "    prediction = {\"seizure_present\":seizure_present,\"seizure_confidence\":seizure_confidence,\n",
    "                   \"onset\":onset,\"onset_confidence\":onset_confidence,\"offset\":offset,\n",
    "                   \"offset_confidence\":offset_confidence}\n",
    "  \n",
    "    return prediction # Dictionary mit prediction - Muss unverändert bleiben!\n",
    "                               \n",
    "                               \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c40aed9-d108-424c-9c07-0b26ea5a187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--test_dir TEST_DIR]\n",
      "                             [--model_name MODEL_NAME] [--allow_fail]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jupyter-wki_team_6/.local/share/jupyter/runtime/kernel-aa4a5be9-86b8-493e-8cf6-dc1c344b273a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3377: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Diese Datei sollte nicht verändert werden und wird von uns gestellt und zurückgesetzt.\n",
    "\n",
    "Skript testet das vortrainierte Modell\n",
    "\n",
    "\n",
    "@author: Maurice Rohr\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from predict_cnn import predict_labels\n",
    "from wettbewerb import EEGDataset, save_predictions\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Predict given Model')\n",
    "    parser.add_argument('--test_dir', action='store',type=str,default='../../test/')\n",
    "    parser.add_argument('--model_name', action='store',type=str,default='model.json')\n",
    "    parser.add_argument('--allow_fail',action='store_true',default=False)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Erstelle EEG Datensatz aus Ordner\n",
    "    dataset = EEGDataset(args.test_dir)\n",
    "    print(f\"Teste Modell auf {len(dataset)} Aufnahmen\")\n",
    "    \n",
    "    predictions = list()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Rufe Predict Methode für jedes Element (Aufnahme) aus dem Datensatz auf\n",
    "    for item in dataset:\n",
    "        id,channels,data,fs,ref_system,eeg_label = item\n",
    "        try:\n",
    "            _prediction = predict_labels(channels,data,fs,ref_system,model_name=args.model_name)\n",
    "            _prediction[\"id\"] = id\n",
    "            predictions.append(_prediction)\n",
    "        except:\n",
    "            if args.allow_fail:\n",
    "                raise\n",
    "        \n",
    "    pred_time = time.time()-start_time\n",
    "    \n",
    "    save_predictions(predictions) # speichert Prädiktion in CSV Datei\n",
    "    print(\"Runtime\",pred_time,\"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2efd7098-8e66-49ef-b1a4-8fcfd53cb7a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (781199126.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [4]\u001b[0;36m\u001b[0m\n\u001b[0;31m    seizure_present_array = np.zeros(data_tensor.shape[0], dtype=bool)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Bereiten Sie die Struktur für die Erfassung der Vorhersagen vor\n",
    "    seizure_present_array = np.zeros(data_tensor.shape[0], dtype=bool)\n",
    "\n",
    "    # Vorhersagen durchführen und überprüfen, ob ein Anfall vorhanden ist\n",
    "    with torch.no_grad():\n",
    "        predictions = cnn_classifier(data_tensor)  # Vorhersagen für das gesamte Batch\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)\n",
    "        seizure_indices = torch.where(predicted_classes == 1)[0]  # Angenommen, Klasse 1 steht für \"seizure\"\n",
    "\n",
    "        if len(seizure_indices) > 0:\n",
    "            seizure_present = [1]\n",
    "            # Erster Anfallindex kann als Anfallsbeginn betrachtet werden\n",
    "            first_seizure_index = seizure_indices[0].item()\n",
    "            onset = N_samples * first_seizure_index / fs  # Berechnung des Anfallsbeginns in Sekunden\n",
    "            offset = N_samples * seizure_indices[-1].item() / fs  # Berechnung des Anfallsendes\n",
    "            # Annahme: Confidence-Werte sind statisch, könnten dynamisch angepasst werden\n",
    "            seizure_confidence = 0.8  # Beispielwert, anpassen basierend auf Ihrer Modellausgabe oder Heuristik\n",
    "            offset_confidence = 0.8  # Beispielwert, anpassen basierend auf Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d785bea-057e-4f1b-9c2c-3ec00de5854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python predict_pretrained.py --test_dir ../../test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b5ec3-25c9-486a-b1c9-67161d0acf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "python score.py --test_dir ../../test/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
