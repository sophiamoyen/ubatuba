{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b67f99ec-1aab-40ad-9fe1-5af3fcde16e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\t Dateien wurden geladen.\n"
     ]
    }
   ],
   "source": [
    "ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references('../../shared_data/training_mini/') \n",
    "# Importiere EKG-Dateien, zugeh√∂rige Diagnose, Sampling-Frequenz (Hz) und Name (meist fs=256 Hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b05110-2ca8-42a1-9a3c-82eff94d6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from wettbewerb import load_references, get_3montages\n",
    "import mne\n",
    "from scipy import signal as sig\n",
    "import ruptures as rpt\n",
    "import json\n",
    "import pywt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import numpy as np\n",
    "from wettbewerb import load_references, get_3montages, get_6montages\n",
    "import pywt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "number_montages = 3\n",
    "N_samples = 2000 # Number of samples per division\n",
    "# Decompose the wave\n",
    "wavelet = 'db4'\n",
    "scaler = StandardScaler()\n",
    "new_signal = []\n",
    "\n",
    "mont1_signal = []\n",
    "mont2_signal = []\n",
    "mont3_signal = []\n",
    "whole_mont = [mont1_signal,mont2_signal,mont3_signal]\n",
    "for i,_id in enumerate(ids):\n",
    "    \n",
    "    if number_montages == 6:\n",
    "        _montage, _montage_data, _is_missing = get_6montages(channels[i], data[i])\n",
    "    else:\n",
    "        _montage, _montage_data, _is_missing = get_3montages(channels[i], data[i])\n",
    "        \n",
    "    _fs = sampling_frequencies[i]\n",
    "    features_per_id = []\n",
    "\n",
    "    for j, signal_name in enumerate(_montage):\n",
    "        signal = _montage_data[j]\n",
    "        # Notch-Filter to compensate net frequency of 50 Hz\n",
    "        signal_notch = mne.filter.notch_filter(x=signal, Fs=_fs, freqs=np.array([50.,100.]), n_jobs=2, verbose=False)\n",
    "        # Bandpassfilter between 0.5Hz and 70Hz to filter out noise\n",
    "        signal_filter = mne.filter.filter_data(data=signal_notch, sfreq=_fs, l_freq=0.5, h_freq=70.0, n_jobs=2, verbose=False)\n",
    "        # Defining number of divisions for signal\n",
    "        N_div = len(signal_filter)//N_samples\n",
    "        # Normalizing data\n",
    "        norm_montage_data = scaler.fit_transform(signal_filter.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    \n",
    "        for i in range(N_div):\n",
    "            montage_array = norm_montage_data[i*N_samples:(i+1)*N_samples]\n",
    "            whole_mont[j].append(montage_array)\n",
    "\n",
    "\n",
    "labels = []\n",
    "for i,_id in enumerate(ids):\n",
    "    if eeg_labels[i][0]:\n",
    "        onset = eeg_labels[i][1]\n",
    "        offset = eeg_labels[i][2]\n",
    "        sample_freq = sampling_frequencies[i]\n",
    "        total_time = len(data[i][1])/sample_freq\n",
    "        N_div = len(data[i][1])//N_samples\n",
    "        for num in range(N_div):\n",
    "            if (((total_time/N_div)*(num) <= onset) and ((total_time/N_div)*(num+1) > onset)) or (((total_time/N_div)*(num) >= onset) and ((total_time/N_div)*(num) < offset)):\n",
    "                labels.append([1])\n",
    "            else:\n",
    "                labels.append([0])\n",
    "    else:\n",
    "        N_div = len(data[i][1])//N_samples\n",
    "        for num in range(N_div):\n",
    "            labels.append([0])\n",
    "labels = np.reshape(labels, (1,-1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0daa0cf2-d501-42e9-abd0-6307b889e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "oversample = SMOTE()\n",
    "undersample = RandomUnderSampler()\n",
    "\n",
    "mont1_signal, labels1 = undersample.fit_resample(whole_mont[0],labels)\n",
    "mont2_signal, labels2 = undersample.fit_resample(whole_mont[1],labels)\n",
    "mont3_signal, labels3 = undersample.fit_resample(whole_mont[2],labels)\n",
    "\n",
    "whole_mont_sampled = [mont1_signal,mont2_signal,mont3_signal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7d5da4e-620a-4715-be34-6fa51e0f8b9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.optimizers.Adam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAdam\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.optimizers.Adam'"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "import tensorflow.keras.optimizers.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daba6775-d9a0-4f9d-a805-6705ace23c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1412"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mont1_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a977c1fe-d91c-493b-9a06-0697697071a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([706, 706]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels1, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0993459-778f-4ea9-9613-b4a52bac1b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scales = range(1,128)\n",
    "waveletname = 'morl'\n",
    "train_size = len(labels1)\n",
    "\n",
    "train_data_cwt = np.ndarray(shape=(train_size, 127, 127, 9))\n",
    "\n",
    "for i in range(0,train_size):\n",
    "    for j in range(len(whole_mont_sampled)):\n",
    "        signal = whole_mont_sampled[j][i]\n",
    "        coeff, freq = pywt.cwt(signal, scales, waveletname, 1)\n",
    "        coeff_ = coeff[:,:127]\n",
    "        train_data_cwt[i, :, :, j] = coeff_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07c2c3c-3146-4a78-b899-2dc5effd11ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1412, 127, 127, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_data_cwt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0868d207-5a7c-4b12-addb-697f5d82521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data_cwt[:1200,:,:,:]\n",
    "x_test = train_data_cwt[1200:,:,:,:]\n",
    "y_train = labels1[:1200]\n",
    "y_test = labels1[1200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41bacee8-0c82-4f2f-a21b-dd0b71fbb2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94fb2e64-c44f-4700-9f2d-1ed931f519f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_x = 127\n",
    "img_y = 127\n",
    "img_z = 3\n",
    "input_shape = (img_x, img_y, img_z)\n",
    " \n",
    "#num_classes = 6\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    " \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    " \n",
    "#y_train = keras.utils.np_utils.to_categorical(y_train)\n",
    "#y_test = keras.utils.np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f8de2-9feb-4b6d-b64b-0a8770e9513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22444035-1169-4bd4-997d-eacb2985811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d7737-d0c1-4674-ba6b-b7734b96d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f33d2d-0a0a-49fb-af54-6dbe8341dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c71793-0311-46b6-9b2d-e1b723a9d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "history = History()\n",
    " \n",
    "img_x = 127\n",
    "img_y = 127\n",
    "img_z = 9\n",
    "input_shape = (img_x, img_y, img_z)\n",
    " \n",
    "num_classes = 6\n",
    "batch_size = 16\n",
    "num_classes = 7\n",
    "epochs = 10\n",
    " \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    " \n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[history])\n",
    " \n",
    "train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3b513-8fbb-45de-9e24-03695136aa29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
